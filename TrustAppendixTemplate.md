# Trust Appendix: [Paper Title]

Provided only with the final submission: [Author Name] [Department] [Institution] [Email] [Phone]

**Trust (aka, Computational Results Analysis) Appendix:** [**sc-trust-appendix-20180131.zip**](https://collegeville.github.io/sc-reproducibility/sc-trust-appendix-20180131.zip)

**Instructions: ** _This form enables authors to provide details about verification and validation efforts that help improve the trustworthiness of their computational results. We are interested in techniques that authors use for pre-execution, peri-execution and post-execution analysis._

**Objective: ** _Use this template to produce a 1 – 2 page appendix for your submission. Describe the approaches you use to improve trustworthiness [1] of your results. Provide an analysis of your results that demonstrate correct execution._

**Examples:** _ Some types of diagnostics could be:_

- _Validate timers (time something with a known execution time, determine the precision and statistical variability of the timer)._
- _Confirm results for a manufactured solution, executed before or during the production simulation._
- _Test analytics of the problem, e.g., generate a problem with known spectral properties and test its behavior._

– Remove the above section before submitting –

**Outline**

**A. Trust Appendix: [Paper Title]**

**A.1 Abstract**

- _Overall description of your approach and how trustworthiness of your results is improved._
- _For example: How you validated timers, what manufactured solution or spectral properties you leveraged, etc._

**A.2  Results Analysis Discussion**

- _Description of results, their correctness and any concerns about them. If your paper is only about performance, describe how you assure the quality of performance measurements and that you have preserved correct computational results._

**A.3  Summary**

- _Final summary demonstrating the trustworthiness of your results._

**A.4 Notes**

_Optional comments._

-----------------------
[1] We use the term trustworthiness to indicate a broad category of ways that can be used to demonstrate that your software is behaving the way you intend it to behave. We expect that most of the approaches you will use are focused on _verification, _ensuring that your software computes the answer you expect it to compute (doing things right). But we leave open the possibility that you also do some validation testing (doing the right thing).
